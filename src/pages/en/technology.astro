---
import Layout from '~/layouts/PageLayout.astro';
import Content from '~/components/widgets/Content.astro';
import Stats from '~/components/widgets/Stats.astro';
import Steps from '~/components/widgets/Steps.astro';
import heroImage from '~/assets/images/hero-image-3.png';

const metadata = {
  title: 'Technology & Research',
  description: 'Dive into the technical details of Moxin-LLM, from its architecture and training data to our reinforcement learning process.',
};
---

<Layout metadata={metadata}>
  <section class="px-4 py-16 sm:py-20">
    <div class="mx-auto max-w-6xl">
      <div class="flex flex-col md:flex-row items-center gap-8">
        <div class="md:w-1/2">
          <img
            class="rounded-md w-full"
            src={heroImage.src}
            alt="Moxin LLM Technology"
            width={700}
            height={500}
          />
        </div>
        <div class="md:w-1/2 text-center md:text-left">
          <h1 class="text-4xl font-bold tracking-tight sm:text-5xl">
            Technology & Research
          </h1>
          <p class="mt-4 text-xl text-muted">
            Moxin-LLM is built on a foundation of complete transparency. We adhere to the "open science" principles of the Model Openness Framework (MOF), releasing our training code, data, and checkpoints to foster a more inclusive and collaborative research environment. 
          </p>
        </div>
      </div>
    </div>
  </section>

  <Stats
    title="Moxin-LLM at a Glance"
    stats={[
      { title: 'Tokens Trained', amount: '2T+' },
      { title: 'Context Length', amount: '32K' },
      { title: 'Training Cost', amount: '$160,000' },
      { title: 'Openness Level', amount: 'MOF Open Science' },
    ]}
  />

  <Content
    tagline="Core Architecture"
    title="Building a Better Foundation"
    items={[
      {
        title: 'Enhanced Mistral Base',
        description: 'We extend the Mistral architecture with 36 transformer blocks (up from 32) to improve learning capacity.  This avoids the restrictive licenses and data contamination issues associated with other models. ',
      },
      {
        title: 'Long-Context Efficiency',
        description: 'Using Sliding Window Attention (SWA) and a Rolling Buffer Cache, our model supports a 32K context length while reducing memory usage by ~8x compared to standard methods. ',
      },
      {
        title: 'Innovative MoE Tokenizer',
        description: 'A unique Mixture-of-Experts (MoE) structure at the tokenizer level provides enhanced, efficient support for multiple languages, including Chinese, Japanese, and Korean, beyond just Latin characters. ',
      },
    ]}
  />

  <Content
    isReversed
    tagline="Data & Training"
    title="Quality Data for Superior Performance"
    items={[
      {
        title: 'High-Quality Text Corpus',
        description: 'Our text data is a mix of SlimPajama (a cleaned, deduplicated RedPajama version) and DCLM-BASELINE, which uses quality filters to retain only the top 10% of web documents. ',
      },
      {
        title: 'Reasoning through Code',
        description: 'We incorporate the-stack-dedup dataset, which includes code from 358 programming languages.  This not only enables code generation but also improves the model\'s overall logical reasoning. ',
      },
      {
        title: 'Phased Training Approach',
        description: 'The model undergoes a three-phase training process, starting with a 2K context, extending to 4K, and finishing with a capability enhancement phase that integrates high-quality data from evaluation benchmarks. ',
      },
    ]}
  />

  <Steps
    tagline="From Assistant to Reasoner"
    title="A Disciplined Path to Advanced Reasoning"
    items={[
      {
        title: 'Step 1: Supervised Fine-Tuning (SFT)',
        description: 'The base model is first fine-tuned using the open-source TÃ¼lu 3 framework on a diverse data mixture to create Moxin-Instruct, a helpful and harmless AI assistant. ',
        icon: 'tabler:school',
      },
      {
        title: 'Step 2: Direct Preference Optimization (DPO)',
        description: 'The SFT model is further trained with DPO on a preference dataset, aligning it more closely with user intent and preferred response styles. ',
        icon: 'tabler:user-check',
      },
      {
        title: 'Step 3: Reinforcement Learning (GRPO)',
        description: 'To create Moxin-Reasoning, we apply Group Relative Policy Optimization (GRPO), a pure RL method inspired by DeepSeek, to dramatically enhance Chain-of-Thought capabilities. ',
        icon: 'tabler:flame',
      },
      {
        title: 'Result: SOTA Reasoning in a 7B Model',
        description: 'The outstanding performance of Moxin-Reasoning demonstrates that advanced RL techniques can be highly effective for smaller 7B models, achieving results previously seen only in much larger models. ',
        icon: 'tabler:award',
      },
    ]}
  />

</Layout>