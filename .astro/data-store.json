[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.7.3","content-config-digest","e7304cbed3c9019b","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://moxin-org.github.io\",\"compressHTML\":true,\"base\":\"/moxin-llm-web/\",\"trailingSlash\":\"always\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image/\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"cdn.pixabay.com\"],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null,null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","post",["Map",11,12],"putting-open-to-the-test",{"id":11,"data":13,"body":25,"filePath":26,"digest":27,"rendered":28},{"publishDate":14,"title":15,"excerpt":16,"image":17,"category":18,"tags":19},["Date","2025-06-06T00:00:00.000Z"],"Putting \"Open\" to the Test with the Model Openness Framework","The AI world is buzzing with \"open-source\" models, but how can we verify these claims without getting lost in the hype? This is where the Model Openness Framework (MOF) steps in.","https://images.unsplash.com/photo-1516996087931-5ae405802f9f?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2070&q=80","Tutorials",[20,21,22,23,24],"MOF","AI Transparency","Reproducibility","Open-washing","AI Model Evaluation","The AI world is buzzing with \"open-source\" models, but let's be honest – \"open\" can mean a lot of different things. Sometimes it means you get the code, sometimes just the weights, and often it comes with licenses that need a lawyer to understand. Amidst this, Moxin LLM has stepped onto the scene, making a bold claim: it's not just a high-performance model, but truly open and fully reproducible.\n\nMoxin aims to deliver state-of-the-art performance, especially on edge devices, while providing deep transparency with its GPRO and Tokenizer MoE architecture. But how can we verify these claims without getting lost in the hype?\n\nThis is where the Model Openness Framework (MOF) steps in. MOF is a standardized system designed to evaluate just how \"open\" an AI model really is. It looks at everything from code and data to documentation and licenses, cutting through ambiguity. Let's see how Moxin LLM stacks up.\n\n## What Makes Moxin Tick?\n\nMoxin LLM is a family of models (like Moxin-7B-Base and Moxin-7B-Chat) built with some key principles:\n\n-   **Radical Openness**: They plan to release model weights, details on training data, and the scripts used, allowing deep dives and custom builds.\n-   **Top-Tier Performance**: It aims for SOTA results, comparable to well-known models.\n-   **Edge-Ready**: Designed to run efficiently on your local devices.\n-   **Clear Licensing**: It uses the Apache-2.0 license, a standard, permissive open-source license. This is a big plus for clarity compared to custom licenses.\n-   **Ecosystem**: It's part of a larger stack, including an inference engine (OminiX) and developer tools.\n\nMoxin explicitly wants to lead in transparency and follow the MOF framework. So, let's hold them to it.\n\n## Moxin's MOF Report Card\n\nMOF uses a three-tier system: Class III (Open Model), Class II (Open Tooling), and Class I (Open Science), with Class I being the most open. Based on Moxin's stated goals and releases, here’s a likely evaluation:\n\n| MOF Class | Components Included | Moxin LLM |\n| :--- | :--- | :---: |\n| **Class I. Open Science** | **Intermediate Model Parameters** | ❌ |\n| | **Datasets** | ❌ |\n| | **Data Preprocessing Code** | ✔️ |\n| | **Research Paper** | ✔️ |\n| | **Model Metadata (optional)** | ✔️ |\n| | *All Class II and III Components* | |\n| **Class II. Open Tooling**| **Training, Validation, and Testing Code**| ✔️ |\n| | **Evaluation Code** | ❌ |\n| | **Evaluation Data** | ✔️ |\n| | **Supporting Libraries & Tools** | ✔️ |\n| | **Inference Code** | ✔️ |\n| | *All Class III Components* | |\n| **Class III. Open Model**| **Data Card** | ❌ |\n| | **Model Card** | ✔️ |\n| | **Final Model Parameters** | ✔️ |\n| | **Model Architecture** | ✔️ |\n| | **Technical Report or Research Paper**| ✔️ |\n| | **Evaluation Results** | ✔️ |\n| | **Sample Model Outputs (optional)**| ❌ |\n\n*(Note: ✔️ = Likely released/planned with open license; ❌ = Likely not released/optional).*\n\nThis places Moxin LLM firmly in **Class II (Open Tooling)**, and it's knocking on the door of **Class I**.\n\nWhat does this mean? It means Moxin isn't just handing over a black box (Class III). By providing weights, architecture, code (inference & training), and using Apache-2.0, they're giving developers the tools to use, understand, and rebuild significant parts of the system. This is a strong commitment to transparency and usability. While full datasets and intermediate parameters (Class I) remain elusive (a common challenge due to cost and data rights ), Moxin's score is impressive and largely validates its \"truly open\" claims.\n\n## Why Should AI Builders Care About MOF?\n\nUsing MOF isn't just an academic exercise; it's good practice and good marketing for any project aiming for openness.\n\n-   **Builds Trust**: In an era of \"open-washing\", MOF provides a clear, objective measure. A high MOF score tells users, \"We mean it when we say we're open.\"\n-   **Sets Expectations**: It clearly defines what users get – the code, the weights, the data info, the licenses. No more guesswork.\n-   **Encourages Community**: True openness, verifiable by MOF, empowers developers to study, build upon, and contribute back to models, fostering a vibrant ecosystem.\n-   **Provides a Goal**: MOF gives model producers a clear roadmap for increasing their transparency and achieving higher levels of openness.\n\nMoxin LLM is setting a strong example by embracing transparency and aligning with the MOF. We encourage other model producers to do the same. Evaluate your models, publish your MOF scores, and use it as a tool to proudly showcase how open you truly are. It helps users, builds trust, and ultimately pushes the entire field of AI forward.","src/data/post/putting-open-to-the-test.md","f9927b2413d23960",{"html":29,"metadata":30},"\u003Cp>The AI world is buzzing with “open-source” models, but let’s be honest – “open” can mean a lot of different things. Sometimes it means you get the code, sometimes just the weights, and often it comes with licenses that need a lawyer to understand. Amidst this, Moxin LLM has stepped onto the scene, making a bold claim: it’s not just a high-performance model, but truly open and fully reproducible.\u003C/p>\n\u003Cp>Moxin aims to deliver state-of-the-art performance, especially on edge devices, while providing deep transparency with its GPRO and Tokenizer MoE architecture. But how can we verify these claims without getting lost in the hype?\u003C/p>\n\u003Cp>This is where the Model Openness Framework (MOF) steps in. MOF is a standardized system designed to evaluate just how “open” an AI model really is. It looks at everything from code and data to documentation and licenses, cutting through ambiguity. Let’s see how Moxin LLM stacks up.\u003C/p>\n\u003Ch2 id=\"what-makes-moxin-tick\">What Makes Moxin Tick?\u003C/h2>\n\u003Cp>Moxin LLM is a family of models (like Moxin-7B-Base and Moxin-7B-Chat) built with some key principles:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Radical Openness\u003C/strong>: They plan to release model weights, details on training data, and the scripts used, allowing deep dives and custom builds.\u003C/li>\n\u003Cli>\u003Cstrong>Top-Tier Performance\u003C/strong>: It aims for SOTA results, comparable to well-known models.\u003C/li>\n\u003Cli>\u003Cstrong>Edge-Ready\u003C/strong>: Designed to run efficiently on your local devices.\u003C/li>\n\u003Cli>\u003Cstrong>Clear Licensing\u003C/strong>: It uses the Apache-2.0 license, a standard, permissive open-source license. This is a big plus for clarity compared to custom licenses.\u003C/li>\n\u003Cli>\u003Cstrong>Ecosystem\u003C/strong>: It’s part of a larger stack, including an inference engine (OminiX) and developer tools.\u003C/li>\n\u003C/ul>\n\u003Cp>Moxin explicitly wants to lead in transparency and follow the MOF framework. So, let’s hold them to it.\u003C/p>\n\u003Ch2 id=\"moxins-mof-report-card\">Moxin’s MOF Report Card\u003C/h2>\n\u003Cp>MOF uses a three-tier system: Class III (Open Model), Class II (Open Tooling), and Class I (Open Science), with Class I being the most open. Based on Moxin’s stated goals and releases, here’s a likely evaluation:\u003C/p>\n\u003Cdiv style=\"overflow:auto\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth align=\"left\">MOF Class\u003C/th>\u003Cth align=\"left\">Components Included\u003C/th>\u003Cth align=\"center\">Moxin LLM\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd align=\"left\">\u003Cstrong>Class I. Open Science\u003C/strong>\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Intermediate Model Parameters\u003C/strong>\u003C/td>\u003Ctd align=\"center\">❌\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Datasets\u003C/strong>\u003C/td>\u003Ctd align=\"center\">❌\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Data Preprocessing Code\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Research Paper\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Model Metadata (optional)\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cem>All Class II and III Components\u003C/em>\u003C/td>\u003Ctd align=\"center\">\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003Cstrong>Class II. Open Tooling\u003C/strong>\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Training, Validation, and Testing Code\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Evaluation Code\u003C/strong>\u003C/td>\u003Ctd align=\"center\">❌\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Evaluation Data\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Supporting Libraries &#x26; Tools\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Inference Code\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cem>All Class III Components\u003C/em>\u003C/td>\u003Ctd align=\"center\">\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003Cstrong>Class III. Open Model\u003C/strong>\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Data Card\u003C/strong>\u003C/td>\u003Ctd align=\"center\">❌\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Model Card\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Final Model Parameters\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Model Architecture\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Technical Report or Research Paper\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Evaluation Results\u003C/strong>\u003C/td>\u003Ctd align=\"center\">✔️\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd align=\"left\">\u003C/td>\u003Ctd align=\"left\">\u003Cstrong>Sample Model Outputs (optional)\u003C/strong>\u003C/td>\u003Ctd align=\"center\">❌\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\u003C/div>\n\u003Cp>\u003Cem>(Note: ✔️ = Likely released/planned with open license; ❌ = Likely not released/optional).\u003C/em>\u003C/p>\n\u003Cp>This places Moxin LLM firmly in \u003Cstrong>Class II (Open Tooling)\u003C/strong>, and it’s knocking on the door of \u003Cstrong>Class I\u003C/strong>.\u003C/p>\n\u003Cp>What does this mean? It means Moxin isn’t just handing over a black box (Class III). By providing weights, architecture, code (inference &#x26; training), and using Apache-2.0, they’re giving developers the tools to use, understand, and rebuild significant parts of the system. This is a strong commitment to transparency and usability. While full datasets and intermediate parameters (Class I) remain elusive (a common challenge due to cost and data rights ), Moxin’s score is impressive and largely validates its “truly open” claims.\u003C/p>\n\u003Ch2 id=\"why-should-ai-builders-care-about-mof\">Why Should AI Builders Care About MOF?\u003C/h2>\n\u003Cp>Using MOF isn’t just an academic exercise; it’s good practice and good marketing for any project aiming for openness.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Builds Trust\u003C/strong>: In an era of “open-washing”, MOF provides a clear, objective measure. A high MOF score tells users, “We mean it when we say we’re open.”\u003C/li>\n\u003Cli>\u003Cstrong>Sets Expectations\u003C/strong>: It clearly defines what users get – the code, the weights, the data info, the licenses. No more guesswork.\u003C/li>\n\u003Cli>\u003Cstrong>Encourages Community\u003C/strong>: True openness, verifiable by MOF, empowers developers to study, build upon, and contribute back to models, fostering a vibrant ecosystem.\u003C/li>\n\u003Cli>\u003Cstrong>Provides a Goal\u003C/strong>: MOF gives model producers a clear roadmap for increasing their transparency and achieving higher levels of openness.\u003C/li>\n\u003C/ul>\n\u003Cp>Moxin LLM is setting a strong example by embracing transparency and aligning with the MOF. We encourage other model producers to do the same. Evaluate your models, publish your MOF scores, and use it as a tool to proudly showcase how open you truly are. It helps users, builds trust, and ultimately pushes the entire field of AI forward.\u003C/p>",{"headings":31,"localImagePaths":42,"remoteImagePaths":43,"frontmatter":44,"imagePaths":48},[32,36,39],{"depth":33,"slug":34,"text":35},2,"what-makes-moxin-tick","What Makes Moxin Tick?",{"depth":33,"slug":37,"text":38},"moxins-mof-report-card","Moxin’s MOF Report Card",{"depth":33,"slug":40,"text":41},"why-should-ai-builders-care-about-mof","Why Should AI Builders Care About MOF?",[],[],{"publishDate":45,"title":15,"excerpt":16,"image":17,"category":18,"tags":46,"readingTime":47},["Date","2025-06-06T00:00:00.000Z"],[20,21,22,23,24],4,[]]